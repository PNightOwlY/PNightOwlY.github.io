<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-09-24T22:37:59+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Ying Peng Blog</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</subtitle><author><name>Ying Peng</name><email>ying.peng.8170@student.uu.se</email></author><entry><title type="html">Semantic Match Deep Dive 1</title><link href="http://localhost:4000/2023/09/14/semantic-match-deep-dive.html" rel="alternate" type="text/html" title="Semantic Match Deep Dive 1" /><published>2023-09-14T00:00:00+08:00</published><updated>2023-09-14T00:00:00+08:00</updated><id>http://localhost:4000/2023/09/14/semantic-match-deep-dive</id><content type="html" xml:base="http://localhost:4000/2023/09/14/semantic-match-deep-dive.html"><![CDATA[<h1 id="introduction-of-semantic-match">Introduction of Semantic Match</h1>
<p>Semantic match is one of the basic tasks of NLP, the goal of the task is to determine whether two sentences have the same semantic meaning. Semantic match is widely used in natural language processing, such as question answering, information retrieval, machine reading comprehension, etc.</p>

<p>Semantic match tasks can be divided into two categories:</p>
<ul>
  <li>sentence to sentence（S2S）</li>
  <li>sentence to passage（S2P）</li>
</ul>

<h2 id="sentence-to-sentences2s">sentence to sentence（S2S）</h2>
<p>The goal of sentence to sentence is to determine whether two sentences have the same semantic meaning. S2S tasks are widely used in intent recogniton in question answering system, such as Ecommerce, Finance, Medical etc. This task is semantic matching of short texts, such as “How much does it cost to buy a house?” and “How much does it cost to buy a house?”, so this type is relatively easier.</p>

<h2 id="sentence-to-passages2p">sentence to passage（S2P）</h2>
<p>The sentence to passage is to determine whether there is a semantic relationship between two sentences and the passage. Due to the bias in the length of sentences, there will be a certain degree of bias in short sentences in projection space.</p>

<h2 id="semantic-match-models">Semantic Match Models</h2>
<p>There are two common semantic matching models, one is a representation-based semantic matching model, and the other is an interaction-based semantic matching model.</p>

<h3 id="represtation-based-model">Represtation-based model</h3>
<p><img src="/images/semantic_match/sentence-bert.jpg" />
The representation-based semantic matching model is represented by the two-tower model of Sentence-Bert<sup id="fnref:1" role="doc-noteref"><a href="#fn:1" class="footnote" rel="footnote">1</a></sup>. The left side is training process, the sentence first output by Bert, and then pooling to get the vector u, v, and then concatenate u, v, |u-v|, |u-v| is then multiply by a trainable weight to get final result through softmax. The right side is the inference process, use the model to get the vector of A(u) and B(v), and then get the cosine similarity and filtered by certain threshold.</p>

\[o=softmax(W_t(u,,v,|u-v|)), W_t\in R^{3n\times k}\]

<p>There are two implementaion of Sentence Bert, Bi-encoder and Dual-encoder respectively.</p>
<ul>
  <li>Bi-encoder: Compute the query and candidate vector representations with shared transformer encoder, and then compute the cosine similarity between the query and candidate vectors, to determine the similarity of the query and candidate. The typical Bert-like model is M3E, text2vec, BGE.</li>
  <li>Dual-encoder: Compute the query and candidate vector representations with different transformer encoder.</li>
</ul>

<p>The two encoders in the Dual-encoder model have independent parameter spaces and state spaces, the Dual-encoder model can process and extract the features of Query and Candidate more flexibly. The training and inference costs of Dual-encoder models are usually higher than Bi-encoder models.</p>

<h3 id="interaction-based-model">Interaction-based model</h3>
<p><img src="/images/semantic_match/cross-encoder.jpg" />
The interactive matching scheme is as shown on the right, which splices two pieces of text together as a single text for classification. Interactive matching allows two texts to be fully compared, so it performs much better, but it is inefficient in retrieval scenarios due to the on-site inference of vectors is required, and representation-based method can calculate and cache all Candidates in advance. During the retrieval process, only vector  for Query is computed, and then all Candidates are calculate the similarity. However, relatively speaking, the degree of interaction of characteristic formulas is Shallow and generally less effective than interactive.</p>

<h3 id="multi-stage-retrieval">Multi-stage Retrieval</h3>
<p>The more common way is to use the representation-based method to retrieve top-n sentences, and then use interaction-base method match the Query and top-n sentences to get the final ranking results.</p>

<ul>
  <li>retrieval stage: calcuate the cos similarity between Query and all sentence, and pick the top-n sentences as candidates.</li>
  <li>ranking stage: concat the Query and top-n sentences as a single text respectively, and use Cross Encoder to get the score of the text.</li>
</ul>

<h2 id="recommend-thesis">Recommend thesis</h2>
<ol>
  <li>Dense Passage Retrieval for Open-Domain Question Answering<sup id="fnref:2" role="doc-noteref"><a href="#fn:2" class="footnote" rel="footnote">2</a></sup></li>
  <li>RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering<sup id="fnref:3" role="doc-noteref"><a href="#fn:3" class="footnote" rel="footnote">3</a></sup></li>
  <li>Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval<sup id="fnref:4" role="doc-noteref"><a href="#fn:4" class="footnote" rel="footnote">4</a></sup></li>
  <li>HLATR: Enhance Multi-stage Text Retrieval with Hybrid List Aware Transformer Reranking<sup id="fnref:5" role="doc-noteref"><a href="#fn:5" class="footnote" rel="footnote">5</a></sup></li>
</ol>

<h3 id="dense-passage-retrieval">Dense Passage Retrieval</h3>
<p><img src="/images/semantic_match/dpr.jpg" />
Dense Passage Retrieval，use dual-encoder encode query and passages respectively，and compute similarity then update the model’s weight. The loss is negative log likelihood as following.</p>

\[L(q_i, p_i^{+},p_{i,1}^{-},p_{i,2}^{-},...,p_{i,n}^{-}) = -log\frac{e^{sim(q_i, p_i^{+})}}{sim(q_i, p_i^{+}) + \sum_{j=1}^{n}e^{sim(q_i, p_{i,j}^{-})}}\]

<p>The proposed three way to generate negatives:</p>

<ul>
  <li>random pick</li>
  <li>BM25 to pick top-k，remove the origin answer</li>
  <li>in-batch negatives, batch_size=64，then there is 63 negatvies，except the query’s positive, all can be negative.</li>
</ul>

<h3 id="rocketqa">RocketQA</h3>
<p>In the field of dense passage retrieval, there exists a difference between training and inference. In training, only a subset of samples is selected as negative examples, while during inference, all samples are compared. Additionally, during training, there are often a large number of false negative samples, which can be used as positive samples.</p>

<p>To alleivate these two problems, the author has proposed three optimization strategies: Cross-batch negatives, Denoised Hard Negatives, Data Augmentation.</p>

<ol>
  <li>
    <p><strong>Cross-batch negatives</strong> firstly compute the embeddings of n samples in m GPUs separately, and then diliver to each GPU. Thus, each training sample has m*n-1 negative samples, which is much larger than the in-batch negatives.</p>
  </li>
  <li>
    <p><strong>Denoised Hard Negatives</strong> trains a dual encoder to retrieval negatives, and then train a cross encoder to remove false negatives. In this way, the negatives could be right possible, which is a good way to do data cleaning.</p>
  </li>
  <li>
    <p><strong>Data Augmentation</strong> uses the cross encoder to label the unlabeled data, which is a semi-supervised learning method, to enlarge the data scale.</p>
  </li>
</ol>

<p>This approach performs advanced on evaluation datasets, but it places high demands on computational resources. For instance, the Cross-batch negatives requires a large number of GPUs. The subsequent training of the dual encoder of the dual encoder also involves a multi-stage process, in curring relatively higher training costs.</p>

<h3 id="cocondense">coCondense</h3>
<p>Condenser is a new pre-training architecture that compresses information into dense vectors through LM pre-training. Most importantly, the authors further propose coCondenser, which adds an unsupervised corpus-level constrastive loss to pre-train paragraph embeddings. It demonstrates performance comparable to RocketQA, the state-of-the-art, carefully designed system. coCondense employs simple small-batch fine-tuning and unsupervised learning, where text snippets are randomly sampled from a document and the model is trained. The objective is to make the embeddings of the CLS token from the same document as similar as possible, while those from different documents should be as dissimilar as possible.</p>

<h3 id="hlatr">HLATR</h3>
<p><img src="/images/semantic_match/hlatr.jpg" />
First retrieval and then rerank is a common way to do document retrieval, where the focus is often on optimizing individual models in each stage to improve overall retrieval performance. However, there hasn’t been much in-depth research on directly coupling multiple stages together for optimization. The authors propose a lightweight HLATR framework that enables efficient retrieval and validate it on two large datasets. Here, the authors mention that although both models are involved in ranking, they have different focuses. The representation-based model (retriever) leans towards coarse-grained features, while the interaction-based model (interaction) emphasizes the interaction between query and document. Additionally, the authors perform a simple weighted combination, assigning different weights to the recall and ranking stages, which also improves overall recall performance.</p>

<h2 id="references">References</h2>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1" role="doc-endnote">
      <p><a href="https://arxiv.org/pdf/1908.10084.pdf">Sentence-Bert:Sentence Embeddings using Siamese BERT-Networks</a> <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:2" role="doc-endnote">
      <p><a href="https://arxiv.org/abs/2004.04906">Dense Passage Retrieval for Open-Domain Question Answering</a> <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:3" role="doc-endnote">
      <p><a href="https://aclanthology.org/2021.naacl-main.466/">RocketQA: An Optimized Training Approach to Dense Passage Retrieval for Open-Domain Question Answering</a> <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:4" role="doc-endnote">
      <p><a href="https://arxiv.org/pdf/2108.05540.pdf">Unsupervised Corpus Aware Language Model Pre-training for Dense Passage Retrieval</a> <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
    <li id="fn:5" role="doc-endnote">
      <p><a href="https://arxiv.org/pdf/2205.10569.pdf">HLATR: Enhance Multi-stage Text Retrieval with Hybrid List Aware Transformer Reranking</a> <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p>
    </li>
  </ol>
</div>]]></content><author><name>Ying Peng</name><email>ying.peng.8170@student.uu.se</email></author><category term="NLP" /><summary type="html"><![CDATA[A detailed introduction of semantic match on both sentence to sentence(s2s) and sentence to passage(s2p) tasks.]]></summary></entry><entry><title type="html">Notes for ChatGPT Prompt Engineering for Developers</title><link href="http://localhost:4000/posts/2023/07/chatgpt-prompt/" rel="alternate" type="text/html" title="Notes for ChatGPT Prompt Engineering for Developers" /><published>2023-07-26T00:00:00+08:00</published><updated>2023-07-26T00:00:00+08:00</updated><id>http://localhost:4000/posts/2023/07/chatgpt-prompt</id><content type="html" xml:base="http://localhost:4000/posts/2023/07/chatgpt-prompt/"><![CDATA[<h1 id="basic-information">Basic Information</h1>
<p>Course Name: <a href="https://learn.deeplearning.ai/chatgpt-prompt-eng/lesson/1/introduction">ChatGPT Prompt Engineering for Developers</a> <br />
GitHub: <a href="https://github.com/ralphcajipe/chatgpt-prompt-engineering">chatgpt-prompt-engineering</a></p>

<h1 id="two-types-of-large-language-modelsllms">Two types of large language models(LLMs)</h1>
<h2 id="base-llm">Base LLM </h2>
<ul>
  <li>Predicts next word, based on text training data</li>
</ul>

<p><strong>Example1:</strong> <br />
Once Upon a time, there was a unicorn <em>that lived in a magical forest with all her unicorn friends.</em></p>

<h2 id="instruction-tuned-llm">Instruction Tuned LLM</h2>

<ul>
  <li>Tries to follow instructions.</li>
  <li>Fine-tune on instructions and good attempts at following those instructions.</li>
  <li>RLHF: Reinforcement learning with Human Feedback, to make the system better able to be helpful and follow instructions.</li>
  <li>Helpful, honest, harmless.</li>
</ul>

<p><strong>Example 2:</strong> <br />
What is the captial of France?
<em>The captial of France is Paris.</em></p>

<h1 id="guideline-for-prompting">Guideline for Prompting</h1>
<h2 id="principles-of-prompting">Principles of Prompting</h2>
<ul>
  <li>Principle 1: Write clear and specfic instructions
Longer prompt provides much more clarity and context for the model, which can actually lead to more detailed and relevant output.</li>
</ul>

<p><strong>Tactic 1: Use delimiters</strong></p>

<p>Use delimiter to specific the content.</p>
<ul>
  <li>Triple quotes: “””</li>
  <li>Triple backticks: ```</li>
  <li>Triple dashes: —</li>
  <li>Angle brackets: &lt;&gt;</li>
  <li>XML tags:<tag> </tag></li>
</ul>

<p><strong>Tactic 2: Ask for structured output</strong></p>

<p>HTML, JSON</p>

<p><strong>Tactic 3: Chekc whether conditions are satisfied</strong></p>

<p>Check assumptions required to do the task</p>

<p><strong>Tactic 4: Few-shot prompting</strong></p>

<ul>
  <li>
    <p>Give successful examples of completing tasks</p>
  </li>
  <li>
    <p>Then ask model to perform the task</p>
  </li>
</ul>

<h2 id="principle-2-give-the-model-time-to-think">Principle 2: Give the model time to think</h2>

<p><strong>Tactic 1: Specify the steps required to complete a task (ask for output in a specified format)</strong></p>
<ul>
  <li>Step 1: …</li>
  <li>Step 2: …</li>
  <li>…</li>
  <li>Step N: …</li>
</ul>

<p><strong>Tactic 2: Instruct the model to work out its own solution before rushing to a conclusion</strong></p>
<ul>
  <li>Model Limitations: Hallucinations</li>
  <li>Hallucinations: Makes statements that sound plausible but are not true.</li>
  <li>Reducing hallucinations: First find relevant information, then answer the question based on the relevant information. </li>
</ul>

<h1 id="iterative">Iterative </h1>
<p><img src="/images/chatgpt-prompt-engineering/iterative.jpg" />
Tuning the prompt is similar with the model tuning, that you need to adjust the prompt util reaching the expected results.</p>

<p>Iterative process is trying something first, analyzing where the result does not give what you want. Then clarify instructions, give the model more time to think adn refine prompts with a batch of examples.</p>

<h1 id="summary--inferring--transforming">Summary &amp; Inferring &amp; Transforming</h1>

<ul>
  <li>Sentiment  Recognition</li>
  <li>Machine translation</li>
  <li>Named Entity Recognition</li>
  <li>Translation</li>
  <li>Reading Comprehension</li>
  <li>…</li>
</ul>

<p>LLM can do a lot NLP tasks by using the varities of Prompts.</p>

<h1 id="expanding">Expanding</h1>
<p><img src="/images/chatgpt-prompt-engineering/temperature.png" />
Temperature: randomness of the model, when the temperature is low, it generate the reliable results, otherwise the answer become variety as the temperature higher.</p>

<h1 id="chatbot">Chatbot</h1>

<p>messages = [{“role”: “”, “content”:”” }]</p>

<p>Roles: {system, user, assistant}</p>

<p>A full conversation contains dialogs from user and assistant, the system role is to set behavior of assistants.</p>

<h1 id="conclusion">Conclusion</h1>
<ul>
  <li>A good prompt can help the model generate far better response.</li>
  <li>Zero-shot heavily rely on model’s ability of understand and expressing. Few-shot could give some examples that model could follow and give more reasonable answer.</li>
  <li>Chain Of Thought can be helpful on logic problems.</li>
</ul>]]></content><author><name>Ying Peng</name><email>ying.peng.8170@student.uu.se</email></author><category term="[&quot;NLP&quot;]" /><summary type="html"><![CDATA[A tutorial of using prompt.]]></summary></entry><entry><title type="html">Language Model</title><link href="http://localhost:4000/posts/2022/09/language-model/" rel="alternate" type="text/html" title="Language Model" /><published>2022-09-20T00:00:00+08:00</published><updated>2022-09-20T00:00:00+08:00</updated><id>http://localhost:4000/posts/2022/09/language-model</id><content type="html" xml:base="http://localhost:4000/posts/2022/09/language-model/"><![CDATA[<h1 id="word2vec">word2vec</h1>
<p>Word2Vec is a popular algorithm used in Natural Language Processing (NLP) for generating word embeddings. We need Word2Vec for several reasons:</p>

<ol>
  <li>
    <p>Word representation: Word2Vec allows us to represent words as dense vectors in a continuous vector space. This representation captures semantic and syntactic relationships between words, enabling machines to better understand and process natural language.</p>
  </li>
  <li>
    <p>Feature extraction: Word2Vec captures meaningful linguistic features from the input text, such as word similarities and contextual relationships. These features can be used as input for various downstream NLP tasks like sentiment analysis, text classification, machine translation, and named entity recognition, improving their performance.</p>
  </li>
  <li>
    <p>Dimensionality reduction: Word2Vec reduces the high-dimensional space of words into a lower-dimensional space while preserving semantic relationships. This reduction makes the computations more efficient and manageable, especially when dealing with large amounts of text data.</p>
  </li>
  <li>
    <p>Contextual understanding: Word2Vec models, such as Skip-gram and Continuous Bag of Words (CBOW), consider the surrounding words or context of a target word when learning word embeddings. This contextual understanding enables the model to capture word meanings based on their surrounding words and improve the accuracy of semantic relationships.</p>
  </li>
</ol>

<p>Overall, Word2Vec plays a crucial role in various NLP applications by providing efficient word representations, feature extraction capabilities, and improved contextual understanding.</p>

<h2 id="word2vec-models">word2vec models</h2>
<p>The common word2vec models are Skip-gram and Continuous Bag of Words(CBOW), consider the surrounding words or context of a target word when learning word embeddings. Both models have two curcial parameters, context word and center word vectors.</p>
<h3 id="cbow">CBOW</h3>
<p>This method takes the context of each word as the input and tries to predict the word corresponding to the context.
<img src="/images/language-model/cbow.jpg" alt="CBOW" />
The above model takes C context words. When $Wvn$ is used to calculate hidden layer inputs, we take an average over all these C context word inputs.</p>

<p>The input or the context word is a one hot encoded vector of size V. The hidden layer contains N neurons and the output is again a V length vector with the elements being the softmax values.
Let’s get the terms in the picture right:</p>
<ul>
  <li>$Wvn$ is the weight matrix that maps the input x to the hidden layer (V<em>N dimensional matrix)
-$W’nv$ is the weight matrix that maps the hidden layer outputs to the final output layer (N</em>V dimensional matrix)</li>
</ul>

<h3 id="skip-gram">Skip-gram</h3>
<p>This method takes the center words as the input and tries to predict the word corresponding to the center.
<img src="/images/language-model/skip-gram.jpg" />
The above model takes a center word as input and outputs C probability distributions of V probabilities, one for each context word.</p>

<p>The more information related to Hierarchical Softmax and Skip-Gram Negative Sampling can be found <a href="https://arxiv.org/pdf/1411.2738.pdf">here</a>.</p>

<h1 id="seq2seq">seq2seq</h1>
<h2 id="recurrent-neural-networks">Recurrent Neural Networks</h2>
<p><img src="/images/language-model/rnn.jpg" />
The design of RNN solves the continuous input space, such as time continuous and space continuous input. Another difference with feed-forward neural networks is that the output format is also a sequence. Given a sequence of input $X = (X_1,X_2,…,X_T)$, and the standard RNN derives a sequence of outputs $y = (y_1, y_2, …, y_T)$ by the following equations and a more intuitive structure displayed above.</p>

\[H_t = sigmoid(WH_{t-1}+UX_t)\]

\[y_t = VH_t\]

<p>Herein, units H, X, and y represent the hidden, input, and output units, respectively. The parameter W, U, and V are the weights that need to be learned by iterating the loss and backpropagation. The RNN structure is suitable for solving the input with any length since the parameters are predominated by W, U, and V . RNN maintains an activation function for each layer, making the model extremely deep when the input space is enormous. Extremely deep models lead to a series of problems, such as vanishing or exploding gradients, which makes the training of RNN difficult.</p>

<h2 id="long-short-term-memory">Long Short-Term Memory</h2>
<p>The central novel concept of LSTM architecture is the introduction of manipulated gates and short-term memory. These two concepts are excellent solutions to the difficulty of RNN in learning time dependencies beyond a few time steps long. The presence of the LSTM structure successfully solved the problem of gradient vanishing that appears during the training of the vanilla RNN.
<img src="/images/language-model/lstm.jpg" /></p>

<h3 id="forget-gate">Forget gate</h3>
<p>The introduction of forget gates allows the LSTM to reset its state, remember common behavioral patterns, and forget unique behaviors, improving the model’s generality and ability to learn sequential tasks. The information from the previously hidden unit $H_{t-1}$ and current input $X_t$ are passed through the forget gate ($f_t$) and will be rescaled to 0 to 1. The value closer to 0 means forget, and approaching one means to keep. The computation formula is presented below (W and b represent weights and bias, respectively).</p>

\[f_t = \sigma(W_f(H_{t-1}, X_t) + b_f)\]

<h3 id="input-gate">Input gate</h3>

<p>The design of the input gate is used to update the cell state. The input gate decides which values of $H_{t-1}$ and $X_t$ need to be updated for computing the new cell state $C_t$. The hidden state $H_{t-1}$ and input $X_t$ information are passed into the tanh function and rescaled between -1 and 1 to help regulate the model. Then we multiply the outputs from the sigmoid and the tanh functions. The output of the sigmoid function decides which information is essential to keep from the tanh output. The new cell state is the summation of the forget and input gate. The following equations describe the whole process.</p>

\[i_t = \sigma(W_i(H_{t-1}, X_t) + b_i)\]

\[C^{'}_t = tanh(W_c(H_{t-1}, X_t) + b_c)\]

\[C_t = f_t C_{t-1} + i_t C^{'}_t\]

<h3 id="output-gate">Output gate</h3>

<p>The output gate determines the value of the following hidden state $H_t$, and $H_t$ contains the value of the previous input, so the value of $H_t$ can also be used to predict. The previous information $H_{t-1}$ and current input $X_t$ are passed into a sigmoid function, and then the updated cell state C is squeezed by the tanh function between -1 to 1. Then we multiply the output from the tanh and the sigmoid, and the outcome is $H_t$, then $C_t$ and $H_t$ move to the next time step. The corresponding formulas are as follows.</p>

\[o_t = \sigma(W_o(H_{t-1}, X_t) + b_o)\]

\[H_t = o_t tanh(C_t)\]

<h2 id="transformers">Transformers</h2>
<p>The Transformers model is a powerful and popular approach in the field of natural language processing (NLP). At the core of the Transformers model lies the self-attention mechanism, which enables the model to handle dependencies between different positions in the input sequence simultaneously. This capability makes Transformers highly effective in processing long text sequences and modeling semantic relationships.</p>

<p>Firstly, the inputs will be transfer to input embedding, then add positional encoding as input of encode part. The encode part consist of two layer <code class="language-plaintext highlighter-rouge">Multi-Head Attention</code> and <code class="language-plaintext highlighter-rouge">Feed Forward</code>, and both with residual connection.  <code class="language-plaintext highlighter-rouge">Multi-head</code> learns more aspects of inputs, and multi-layer makes each layer learn different level attention representation. Decode module almost have the same structure except the masked attetion, since the model is not supposed to see the full outputs.</p>

<p><img src="/images/language-model/transformers.jpg" /></p>

<p>One of the most renowned Transformer models is BERT (Bidirectional Encoder Representations from Transformers), which is a pre-trained language model used for various NLP tasks such as text classification, named entity recognition, sentiment analysis, and more. BERT achieved breakthrough results in natural language understanding tasks and has been widely adopted both in industry and academic research.</p>]]></content><author><name>Ying Peng</name><email>ying.peng.8170@student.uu.se</email></author><category term="NLP" /><summary type="html"><![CDATA[This is a note for deep language models. The contents include introduction of word2vec, seq2seq, transformer models.]]></summary></entry><entry><title type="html">Design and Implementation of Automated Orchestration Architecture</title><link href="http://localhost:4000/2021/06/15/Design-and-Implementation-of-Automated-Orchestration-Architecture.html" rel="alternate" type="text/html" title="Design and Implementation of Automated Orchestration Architecture" /><published>2021-06-15T00:00:00+08:00</published><updated>2021-06-15T00:00:00+08:00</updated><id>http://localhost:4000/2021/06/15/Design-and-Implementation-of-Automated-Orchestration-Architecture</id><content type="html" xml:base="http://localhost:4000/2021/06/15/Design-and-Implementation-of-Automated-Orchestration-Architecture.html"><![CDATA[<h2 id="architecture-of-working-environment">Architecture of Working Environment</h2>
<p><img src="/images/orchestration/Architecture.png" />
The framework of our project is shown above. We have 5 VMs in total: One VM as the orchestrationVM, development cluster(several containers in the development VM and a Ray cluster consisting of 2 VMs),production cluster(a Docker Swarm cluster consisting of 2 VMs).</p>

<p>An important part of our framework is the development cluster(Ray cluster). Since the most time-consumingpart of machine learning model training is the selection, tuning and adjustment of model parameters, we intro-duced the Ray cluster. It consists of two parts, a development server(head node) and several parameter servers.We do the feature extraction and selection, model selection in the development server. After we choose thecandidate models, we will make use of the Ray cluster to do the parameters selection and evaluate them.
In this project, docker swarm was used to deploy the services in production VMs, which make the whole systemmore stable and to have a high availability. On the middle right of the figure 1, we deployed development andparameter tuning VMs, we tune the models in Ray Cluster which can speed up the whole process. The bestset of parameters will be used to train different models in different containers, and the best model will push bygit-hooks from development VM to production1 and production2 VMs.</p>

<p><img src="/images/orchestration/celery_architecture.png" />
On the production server, we run multiple containers. The containers guarantee the isolation between tasks.The tasks running in the container include Flask, RabbitMQ and Celery, as well as workers. Flask is a popularPython web service framework. Our web application is developed based in it. In the worker, we predict thegiven data based on the model and get the output. Celery is an asynchronous task queue. It can be used foranything that needs to be run asynchronously. The Broker (RabbitMQ) is responsible for the creation of task2 queues, dispatching tasks to task queues according to some routing rules, and then delivering tasks from taskqueues to workers. The framework of RabbitMQ and Celery is shown above.In our case, we submit a request on a flask-based web application. After receiving the request, the celery clientcalls RabbitMQ to create and assign tasks to workers. After the worker task is completed, the result is storedin the result backend, sent to the web application and displayed.</p>

<h2 id="tools-automation-and-orchestration">Tools, Automation and Orchestration</h2>
<p>In order to facilitate the reproduction and management of the project, we have automated and orchestrated mostof the steps of the project. Generally speaking, Openstack API is responsible for the automation of creatinginstances on Openstack and configuring their basic environment, Ansible is responsible for managing andmonitoring the VMs, the Ansible playbook is responsible for automatically configuring the detailed operatingenvironment and running programs of each instance, the CI/CD technology(git hook) is used to realize theautomatic push of the latest models to the production server. Most of the configurations including ci/cd areorchestrated by Ansible. Jupyternotebook is used to help to edit the code. Part of the construction of Raycluster and Swarm cluster is also completed automatically by Ansible. But some other settings of the clustersneed to be done manually.</p>

<p>Our development cluster is a Ray cluster. It helps us to speed up the training of our models and parameterstuning. We use Docker Swarm to manage our production cluster. It contains 2 VMs. Docker Swarm providesa set of highly available Docker cluster management solutions, which fully supports the standard Docker API,facilitates the management and scheduling of cluster Docker containers, and makes full use of cluster hostresources. Both the production cluster and the development cluster have scalability in our architecture.</p>

<h2 id="github">Github</h2>
<p>If you want to know more details of our works, please check our reports in <a href="https://github.com/PNightOwlY/pred-on-github-population/tree/main">Github</a>.</p>]]></content><author><name>Ying Peng</name><email>ying.peng.8170@student.uu.se</email></author><summary type="html"><![CDATA[Construct a virtual project environment through automated orchestra- tion technology to realize the separation of developer and producer en- vironments, and compare random forests, GDBT and neural network models to predict the accuracy and reasons of Github projects in the process.]]></summary></entry><entry><title type="html">Reinforcement Learning: Deep Q Learing</title><link href="http://localhost:4000/2021/05/26/reinforcement-learning-deep-Q-learning.html" rel="alternate" type="text/html" title="Reinforcement Learning: Deep Q Learing" /><published>2021-05-26T00:00:00+08:00</published><updated>2021-05-26T00:00:00+08:00</updated><id>http://localhost:4000/2021/05/26/reinforcement-learning-deep-Q-learning</id><content type="html" xml:base="http://localhost:4000/2021/05/26/reinforcement-learning-deep-Q-learning.html"><![CDATA[<center>
<img src="/images/dqn/pong.gif" />
</center>
<h1 id="description-of-dqn">Description of DQN</h1>
<p>DQN is an algorithm where a Q learning framework with a neural network proceeds as a nonlinear approximation on a state-value function. Unlike original Q-learning that requires a great amount of time to create a Q-table and memory to save history, which thus has its limitations in a large-scale problem, this novel algorithm has 2 main advantages to overcome these problems, i.e., experience replay and target network.</p>

<p>With replay buffer, DQN stores the history for off-policy learning and extracts samples from the memory randomly. Because in reinforcement learning, the observed data are ordered. Ordered data is prone to overfitting current episodes. So DQN uses experience replay to solve this problem. Experience replay stores all necessary data (state transitions, rewards and actions) for performing Q learning and extracts sample data from the memory randomly to update. It reduces correlation between data.</p>

<p>In addition, DQN is optimized on a target network with fixed parameters updated periodically so that the Q-network is not changed frequently and makes training more stable.</p>

<h1 id="configurations">Configurations</h1>

<p>In our experiments, we mainly tune the following parameters for comparison.</p>
<ul>
  <li>Memory Size: Maximum size of the replay memory.</li>
  <li>lr: Learning rate, used for optimizer.</li>
  <li>γ: Discount factor.</li>
  <li>Using Win Memory: Whether we use another replay buffer, which will explain on section 3.3.</li>
  <li>Decay: Epsilon decay rate</li>
  <li>Average Reward: Evaluate the final model 10 times and average the rewards.</li>
</ul>

<h1 id="cartpole">CartPole</h1>
<p>All the experiments are implemented in Google Colab. The parameters areexplained as follows.
<img src="/images/dqn/ex1.jpg" />
Note that the prefix “*” for is a distinguished result from the first Experiment 1.</p>

<h2 id="discussion">Discussion</h2>
<p>From Figure 1, it can be found that all figures show messy patterns for rewards. Experiment 1 is selected as a baseline for comparison. In Experiment 2, the memory size is larger and the averaged reward is more satisfactory. With more memory, the model tends to select different samples for gradient descent, so that the result will be more robust. Experiment 3 and 4 use different learning rates. Theoretically, a smaller learning rate gives a smooth and slowly-growing curve, and the larger one produces a more curve with violate fluctuation. However, Figure 1c and 1d show little relationship with expected tendency. Experiment 5 chooses a larger gamma that enables the current state to have a more impact on the future state and thus larger gamma leads to a larger reward, which is in line with our result. In experiment 6, we do not notice any distinguished differences caused by train update frequency, which may have to be set on a larger scale to generate a distinction. Anyway, it seems that there are no obvious differences and similarities among figures, and it is difficult for us to summary the regulations.</p>

<h3 id="reward-function">Reward Function</h3>
<p><img src="/images/dqn/f1.jpg" />
As you can see in experiment 1 and *1, the hyperparameters are the same but with different reward functions. In experiment 1, the reward function is simple, the agent gets reward 1 when the game was not done, otherwise, the reward is -1. But in experiment *1, we changed the reward function which is based on the state. When the car is closer to the midpoint, the reward is higher. When the angle between the flag and the horizontal line is closer to 90 degrees, the reward is higher, and vice versa. The results revealed that a good reward function can make a huge difference in performance when it comes to Reinforcement Learning, which can speed up the process of agent learning.</p>

<h1 id="results-of-pong">Results of Pong</h1>
<p>All the experiments are implemented in Google Colab with 2.5 million frames. The parameters are explained as follows.
<img src="/images/dqn/ex2.jpg" /></p>

<h2 id="discussion-1">Discussion</h2>
<p>The curve in the resulting figures may not be a good description of the performance of the current model, because we take the average of the most recent 10 episodes as the score of the current model. So when the experiment is over, we re-evaluated the average value ten times with the saved model. This result will be more representative.
We implement multiple experiments based on the environment Pong-v0. In general, the results are basically satisfactory. The configuration of the model and its performance(Column Average reward) are displayed as Table 2.</p>
<h3 id="replay-memory-size">Replay Memory Size</h3>

<p>Figure 3 visualizes the results of Experiment 1, 2 and 3. It can be observed from 3a that when the replay memory size is 10000, the performance of the model is unstable, comparing with the averaged reward trend in Experiment 3. The reason for the differences is that the larger the experience replay, the less likely you will sample correlated elements, hence the more stable the training of the NN will be. However, a large experience replay requires a lot of memory so the training process is slower. Therefore, there is a trade-off between training stability (of the NN) and memory requirements. In these three experiments, the gamma valued 1, so the model is unbiased but with high variance, and also we have done the Experiment 2 twice, second time is basically satisfactory (as you can see in the graph), but first Experiment 2 were really poor which is almost same with Experiment 3. The result varies a lot among these two experiment due to the gamma equals to 1.</p>

<h3 id="learning-rate">Learning Rate</h3>
<p>Now we discuss how learning rate affects the averaged reward. It is found from Figure 4 that a high learning rate has relatively large volatility on the overall curve, and the learning ability is not stable enough, but the learning ability will be stronger.</p>
<h3 id="win-replay-memory">Win Replay Memory</h3>
<p>Here we try a new way to train our model and create a win replay memory for those frames that our agent gets reward 1. After 0.4 million frames, we start to randomly pick 5 samples from this win memory and then train the model every 5 thousand frames. The idea is for this kind of memory, the loss may vary a lot, so the model will tune the parameters more. But the results show that the performance is basically the same or even worse than that of learning rate = 0.0002.</p>

<h2 id="summary">Summary</h2>
<p>Each experiment takes 4h on Google Colab. We achieve 10-time average reward of 7.9 as the best result that is better than Experiment 1(suggested configuration on Studium), although the result is somewhat random and may be unreproducible. It seems that the models with higher learning rate(0.002) perform better, but its reward influtuates more sharply.</p>

<p><img src="/images/dqn/f2.jpg" />
<img src="/images/dqn/f3.jpg" />
<img src="/images/dqn/f4.jpg" />
<img src="/images/dqn/f5.jpg" /></p>

<h1 id="github">Github</h1>
<p>If you want to know more details of our works, please check our reports in <a href="https://github.com/PNightOwlY/reinforcement-learning-dqn.git">Github</a>.</p>]]></content><author><name>Ying Peng</name><email>ying.peng.8170@student.uu.se</email></author><summary type="html"><![CDATA[A experiment to implement Deep Q Learning, and explore the performance of the model under different set of parameters.]]></summary></entry><entry><title type="html">Music classifcation</title><link href="http://localhost:4000/2020/12/15/music-classification.html" rel="alternate" type="text/html" title="Music classifcation" /><published>2020-12-15T00:00:00+08:00</published><updated>2020-12-15T00:00:00+08:00</updated><id>http://localhost:4000/2020/12/15/music-classification</id><content type="html" xml:base="http://localhost:4000/2020/12/15/music-classification.html"><![CDATA[<h1 id="problem-music-classification">Problem: music classification</h1>
<p>The technical problem is to tell which songs, in a dataset of 200 songs, Andreas Lindholm is going to like (see Figure 1). The data set consists not of the songs themselves, but of high-level features extracted using the web-API from Spotify1. These high-level features describe characteristics such as the acousticness, danceability, energy, instrumentalness, valence and tempo of each song.</p>

<p>To your help, you are provided a training dataset with 750 songs, each of which Andreas has labeled with LIKE or DISLIKE. You are expected to use all the knowledge that you have acquired in the course about classification algorithms, to come up with one algorithm that you think is suited for this problem and which you decide to put ‘in production’.</p>

<h1 id="dataset">Dataset</h1>
<p>The data set to classify is available as songs_to_classify.csv, and the training data is available as training_data.csv on Studium. The columns in these tables represent extracted features, as specified by the header and documented in Table 1. The column “label” in training_data.csv is encoded as 1 = LIKE and 0 = DISLIKE.</p>

<h1 id="result">Result</h1>
<center><img src="/images/sml/GBDT.png" style="float" /></center>
<center><img src="/images/sml/CART.png" style="float" /></center>
<center><img src="/images/sml/ID3.png" style="float" /></center>
<center><img src="/images/sml/KNN.png" style="float" /></center>
<center><img src="/images/sml/LR.png" style="float" /></center>

<h1 id="report">Report</h1>
<object data="/images/pdf/report_ml.pdf" width="1000" height="1000" type="application/pdf" />]]></content><author><name>Ying Peng</name><email>ying.peng.8170@student.uu.se</email></author><summary type="html"><![CDATA[Use basic machine learning such as logistic regression, knn, random forest, abd GBDT models to do the music classification, and won first place in leardboard.]]></summary></entry></feed>